{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050aca40-9879-4b54-a368-19e1ddf2a7c6",
   "metadata": {},
   "source": [
    "**Q1**. What is Gradient Boosting Regression?\n",
    "\n",
    "**Answer**:\n",
    "Gradient Boosting Regression is a machine learning algorithm used for regression tasks. It is an ensemble learning technique that combines multiple weak learners (usually decision trees) to create a more powerful and accurate model. The term \"gradient\" in Gradient Boosting refers to the optimization method used to minimize the errors of the model during training.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "**(I) Base Learners (Weak Learners):** Gradient Boosting uses a collection of weak learners as base models. Typically, decision trees with shallow depth are used as weak learners, often referred to as \"decision stumps.\"\n",
    "\n",
    "**(II) Initialization**: The algorithm starts by initializing the model with a single weak learner, usually a decision stump. This weak learner is fit to the data, and its predictions are used as the initial predictions of the model.\n",
    "\n",
    "**(III) Residual Fitting**: In each iteration, the algorithm fits a new weak learner to the residuals (differences between the target values and the current predictions) of the previous model. This new learner is fitted in a way that minimizes the residuals.\n",
    "\n",
    "**(IV) Weighted Combination**: The predictions from the new weak learner are then combined with the predictions from the previous model using a weighted sum. The weights are typically small learning rates (shrinkage parameter) that control the contribution of each model.\n",
    "\n",
    "**(V) Iterative Process**: This process of fitting a weak learner to the negative gradient (residuals) of the loss function and combining it with the previous model's predictions is repeated iteratively.\n",
    "\n",
    "**(VI) Stopping Criteria**: The algorithm continues to build new weak learners until a predefined number of iterations (number of trees) is reached or until the model performance on a validation set no longer improves.\n",
    "\n",
    "**(VII) Final Prediction**: The final prediction of the Gradient Boosting Regression model is the sum of the predictions from all weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117efdd-e25b-411c-9628-840b0c27cced",
   "metadata": {},
   "source": [
    "**Q2.** Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f0ff8-3991-4d3a-9ec5-e1091e97adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Create a synthetic dataset for regression\n",
    "def generate_dataset():\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(50, 1)  # 50 samples, 1 feature\n",
    "    y = 3 * X.squeeze() + 2 + 0.1 * np.random.randn(50)  # y = 3*X + 2 + noise\n",
    "    return X, y\n",
    "\n",
    "# Step 2: Define the Gradient Boosting algorithm\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        F = np.zeros(n_samples)  # Initial prediction is zero\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute negative gradient (residuals)\n",
    "            residuals = y - F\n",
    "            \n",
    "            # Fit a decision stump to the residuals\n",
    "            model = DecisionStumpRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions\n",
    "            F += self.learning_rate * model.predict(X)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        F = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            F += self.learning_rate * model.predict(X)\n",
    "        return F\n",
    "\n",
    "# Helper class for decision stump (weak learner)\n",
    "class DecisionStumpRegressor:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit a decision stump (simple linear regression)\n",
    "        self.split_value = X.mean()\n",
    "        y_left = y[X < self.split_value].mean()\n",
    "        y_right = y[X >= self.split_value].mean()\n",
    "        self.predict_left = y_left\n",
    "        self.predict_right = y_right\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.where(X < self.split_value, self.predict_left, self.predict_right)\n",
    "\n",
    "# Step 3: Train the model on the dataset\n",
    "X, y = generate_dataset()\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Step 4: Evaluate the model's performance\n",
    "y_pred = gb_regressor.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7355bc-697f-4363-9ec9-7682162c9f80",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters.\n",
    "\n",
    "**Answer**:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a786dd-316c-47fb-8902-aef0a163f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.001]\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5f077-e4dd-4cd8-873c-c4d1a89e334f",
   "metadata": {},
   "source": [
    "**Q4**. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "**Answer**:\n",
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs slightly better than random guessing on a given task. The term \"weak\" in this context does not imply that the model is inherently inferior; rather, it means that the model's performance is only marginally better than random chance.\n",
    "\n",
    "Gradient Boosting is an ensemble learning technique where multiple weak learners are combined to create a strong learner, capable of making highly accurate predictions. The weak learners are typically decision trees with limited depth, often referred to as \"shallow trees\" or \"decision stumps\" (trees with just one split). Each of these weak learners is trained sequentially, and their errors are corrected by subsequent models in the ensemble.\n",
    "\n",
    "1.The boosting process involves the following steps:\n",
    "\n",
    "2.Initially, the first weak learner is trained on the original data.\n",
    "\n",
    "3.The subsequent weak learners are trained on the residual errors (the differences between the actual target values and the predictions of the current ensemble).\n",
    "\n",
    "4.Each weak learner focuses on the mistakes made by its predecessors, trying to correct those errors.\n",
    "\n",
    "5.The predictions of all weak learners are combined using weighted voting or weighted averaging to make the final prediction of the ensemble.\n",
    "\n",
    "The process continues iteratively, with each weak learner aiming to improve the overall performance of the ensemble. The boosting algorithm pays more attention to the data points that were misclassified by the previous models, effectively \"boosting\" their importance during training.\n",
    "\n",
    "By combining the predictions of many weak learners, Gradient Boosting is able to build a powerful model capable of capturing complex relationships in the data. It is a popular and effective technique for a wide range of machine learning tasks, including classification and regression, and is often used in real-world applications due to its robustness and high accuracy. Common implementations of Gradient Boosting include XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3483d23-a22f-4aca-85b4-4f532d745ed0",
   "metadata": {},
   "source": [
    "**Q5**. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "**Answer**:\n",
    "The intuition behind the Gradient Boosting algorithm can be understood through the following steps:\n",
    "\n",
    "**(I) Ensemble Learning:** Gradient Boosting is an ensemble learning technique, which means it combines multiple weak learners to create a strong learner. The idea is that by aggregating the predictions of several weak learners, the ensemble can make more accurate and robust predictions.\n",
    "\n",
    "**(II) Sequential Training**: The weak learners are trained sequentially, one after the other. Each weak learner focuses on correcting the mistakes made by its predecessors, so the subsequent models in the ensemble learn from the errors of the previous ones.\n",
    "\n",
    "**(III) Residuals as Target**: To correct the mistakes made by the previous model, each new weak learner is trained on the residuals (or errors) of the current ensemble's predictions. Initially, the target for the first weak learner is the actual target values. In subsequent iterations, the target becomes the difference between the actual target values and the predictions made by the ensemble up to that point.\n",
    "\n",
    "**(IV) Weighted Voting/Averaging:** The predictions of all weak learners are combined using weighted voting (in classification problems) or weighted averaging (in regression problems) to make the final prediction of the ensemble. The weights are typically determined by the performance of each weak learner, with better-performing models given more influence in the final prediction.\n",
    "\n",
    "**(V) Bias-Variance Tradeoff**: Gradient Boosting aims to strike a balance between the bias and variance of the ensemble. Individual weak learners may have high bias (due to their simplicity), but when combined correctly, they can reduce the overall bias of the ensemble. At the same time, the combination of weak learners helps to reduce the variance of the ensemble, making it less susceptible to overfitting.\n",
    "\n",
    "**(VI) Boosting Effect**: The \"boosting\" in Gradient Boosting refers to the fact that the subsequent models in the ensemble focus more on the data points that were misclassified or poorly predicted by the previous models. This boosting effect gives more attention to the difficult-to-predict instances, improving the overall performance of the ensemble.\n",
    "\n",
    "**(VII) Loss Function and Gradient Descent**: Gradient Boosting is based on the concept of gradient descent optimization. It tries to minimize a loss function, which measures the difference between the actual target values and the predictions made by the ensemble. In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the ensemble's predictions and uses it to update the parameters of the new weak learner.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b750f79-2406-4098-87ae-d541d9d16663",
   "metadata": {},
   "source": [
    "**Q6**. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "\n",
    "**Answer**:The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and iterative manner. Here's a step-by-step explanation of how it creates the ensemble:\n",
    "\n",
    "**(I) Initialize the Ensemble**: The process starts by initializing the ensemble with a weak learner, typically a decision tree with shallow depth (a decision stump with just one split). This first weak learner is trained on the original data, and its predictions serve as the initial predictions for the ensemble.\n",
    "\n",
    "**(II) Compute Residuals:** After the first weak learner is trained and added to the ensemble, the next step is to calculate the residuals (errors) of the ensemble's predictions compared to the actual target values. The residuals represent the discrepancies between the current ensemble's predictions and the true target values.\n",
    "\n",
    "**(III) Train the Next Weak Learner on Residuals**: The next weak learner is trained on the residuals calculated in the previous step. The target values for this weak learner are the residuals themselves. The goal is to fit this weak learner to the errors made by the current ensemble so that it can correct those mistakes.\n",
    "\n",
    "**(IV) Update Ensemble Predictions:** Once the new weak learner is trained, its predictions are combined with the predictions of the existing ensemble. At this point, the ensemble's predictions have been updated to include the corrections made by the latest weak learner.\n",
    "\n",
    "**(V) Compute New Residuals**: After updating the ensemble's predictions, the process of computing residuals is repeated. The new residuals are calculated by comparing the updated predictions with the actual target values.\n",
    "\n",
    "**(VI) Iterate and Add More Weak Learners**: The process of training weak learners, computing residuals, and updating the ensemble's predictions is repeated for a fixed number of iterations or until a stopping criterion is met. Each new weak learner focuses on correcting the mistakes made by the current ensemble, gradually improving the overall performance of the ensemble.\n",
    "\n",
    "**(VII) Combine Predictions**: Finally, the predictions of all the weak learners in the ensemble are combined using weighted voting (for classification problems) or weighted averaging (for regression problems). The weights are determined based on the performance of each weak learner during training, with better-performing models given more influence in the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b545e-0c84-4203-94d5-f1b877f84f12",
   "metadata": {},
   "source": [
    "**Q7**. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "**Answer**:\n",
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the key concepts and mathematical principles behind the algorithm. Here are the steps involved in developing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "**(I) Ensemble Learning and Weak Learners**: Understand the concept of ensemble learning, where multiple weak learners are combined to create a strong learner. Weak learners are simple models that perform slightly better than random guessing. In Gradient Boosting, decision trees with shallow depth are commonly used as weak learners.\n",
    "\n",
    "**(II) Objective Function (Loss Function):** Define the objective function (also known as the loss function) that measures the difference between the actual target values and the predictions of the current ensemble. The goal of Gradient Boosting is to minimize this objective function.\n",
    "\n",
    "**(III) Gradient Descent**: Gradient Boosting is based on the idea of gradient descent optimization. It uses the gradient of the objective function with respect to the ensemble's predictions to update the weak learners' parameters in each iteration. The gradient provides information about the direction and magnitude of the steepest ascent or descent of the function.\n",
    "\n",
    "**(IV) Initialization**: Initialize the ensemble with a weak learner and calculate the initial predictions of the ensemble.\n",
    "\n",
    "**(V) Residual Calculation**: Calculate the residuals (errors) between the actual target values and the current ensemble's predictions. The residuals represent the discrepancies that need to be corrected by the next weak learner.\n",
    "\n",
    "**(VI) Training Weak Learners:** Train a new weak learner on the residuals calculated in the previous step. The weak learner aims to fit the errors made by the current ensemble, so it should learn to correct those mistakes.\n",
    "\n",
    "**(VII) Weighted Update**: Update the ensemble's predictions by combining the predictions of the existing ensemble with the predictions of the newly trained weak learner. The weighted combination is based on the learning rate (or step size) that controls the contribution of each weak learner.\n",
    "\n",
    "**(VIII) Residual Update**: After updating the ensemble's predictions, compute the new residuals by comparing the updated predictions with the actual target values. The new residuals represent the errors that the next weak learner needs to correct.\n",
    "\n",
    "**(IX) Iteration**: Repeat steps 5 to 8 for a fixed number of iterations or until a stopping criterion is met. Each new weak learner focuses on the mistakes made by the current ensemble, gradually improving the overall performance of the ensemble.\n",
    "\n",
    "**(X) Final Prediction:** After all iterations, combine the predictions of all weak learners using weighted voting (classification) or weighted averaging (regression) to make the final prediction of the ensemble.\n",
    "\n",
    "Throughout the construction of the mathematical intuition of Gradient Boosting, the focus is on the optimization of the objective function using gradient descent and the iterative nature of building the ensemble of weak learners to improve predictive performance. Understanding these key concepts lays the foundation for implementing and fine-tuning Gradient Boosting algorithms for various machine learning tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59a158-4fdf-4f14-b0a8-1ab914e08f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
